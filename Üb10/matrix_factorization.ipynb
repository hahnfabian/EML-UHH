{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10\n",
    "\n",
    "Diese Woche werden wir uns mit Matrixfaktorisierung beschäftigen.\n",
    "Dazu verwenden wir einen Datensatz, der Nachrichten-Artikel enthält. Basierend auf den Titeln und Beschreibungen der Nachrichten sollen sie mögliche Themen/Topics ermitteln, unter denen die Artikel zusammengefasst bzw. gruppiert werden können.\n",
    "\n",
    "Die Daten stammen von [kaggle](https://www.kaggle.com/datasets/rmisra/news-category-dataset/discussion/438853)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> list:\n",
    "    \"\"\"Loads a dataset of news articles from a file containing multiple json objects.\n",
    "    Each object is expected to at least contain the keys 'headline' and 'short_description'.\n",
    "    \n",
    "    Returns a list of joined headlines and short descriptions for each article (object).\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the json file containing the news articles.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings where each string is a concatenation of the headline and short description of a news article.\n",
    "    \"\"\"\n",
    "    with open(data_path, 'r') as f:\n",
    "        json_data = [json.loads(line) for line in f]\n",
    "    data = [entry['headline'] + ' ' + entry[\"short_description\"] for entry in json_data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(file_path: str) -> set:\n",
    "    \"\"\"Load stop words from a file where each stop word is separated by a newline character.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file containing the stop words.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of the stop words stripped of any leading/trailing whitespace.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return {line.strip() for line in f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"./News_Category_Dataset_v3.json\")\n",
    "stop_words = load_stop_words(\"./stopwords-en.txt\")\n",
    "\n",
    "print(\"First 10 sentences:\")\n",
    "for line in data[:10]: print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1\n",
    "\n",
    "Zuerst wollen wir die Daten bereinigen, damit die Vorhersage der Themen besser funktioniert.\n",
    "Dazu konvertieren wir die Wörter in jedem Artikel zu Kleinbuchstaben, entfernen Satzzeichen und Zahlen, und entfernen *stopwords*.\n",
    "\n",
    "Die *stopwords* für diese Aufgabe wurden aus dieser [Quelle](https://github.com/stopwords-iso/stopwords-en/blob/master/stopwords-en.txt) entnommen.\n",
    "\n",
    "Implementieren Sie die Funktion `clean_sentence`, die eine Liste von Datenpunkten und eine Menge von *stopwords* übergeben bekommt und die Datenpunkte bereinigt zurückgibt.\n",
    "\n",
    "Die Funktion wird auf Moodle getestet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sentence(sentence: str, stop_words: set) -> str:\n",
    "    \"\"\"Cleans a sentence (string), by converting it to lowercase and removing numbers,\n",
    "    punctuation, and stop words.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The original sentence.\n",
    "        stop_words (set): A set of stop words.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned sentence.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # Remove numbers and punctuation (keep only letters and spaces)\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)  # Replace punctuation with a space\n",
    "    sentence = re.sub(r'\\d+', '', sentence)  # Remove numbers\n",
    "    # Remove extra spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    # Remove stop words\n",
    "    words = sentence.split()\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    # Join cleaned words into a sentence\n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [clean_sentence(sentence, stop_words) for sentence in data]\n",
    "\n",
    "print(\"First 10 cleaned sentences:\")\n",
    "for line in data[:10]: print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization (keine Aufgabe)\n",
    "\n",
    "Wir wollen nun die TF-IDF-Matrix bestimmen, die wir später faktorisieren, um die Topics zu modelieren.\n",
    "\n",
    "Diese Art der Feature-Berechnung kennen Sie bereits aus einer vorherigen Übung und wird daher vorgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(data: list) -> tuple[ArrayLike, ArrayLike]:\n",
    "    \"\"\"Computes the TF-IDF matrix of a text corpus.\n",
    "    Returns the TF-IDF scores and the associated words.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of strings where each string represents a sentence where words\n",
    "        are seperated by whitepsace.\n",
    "\n",
    "    Returns:\n",
    "        ArrayLike: The TF-IDF matrix of the data.\n",
    "        ArrayLike: The feature names associated with the TF-IDF matrix.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    tfidf = vectorizer.fit_transform(data)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return tfidf, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf, words = compute_tfidf(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "\n",
    "Nun soll die eigentliche Matrixfaktorisierung durchgeführt werden, um eine Topic-Zuordnung für die verschiedenen Artikel zu bestimmen.\n",
    "\n",
    "Implementieren Sie die Funktion `compute_matrix_factorization`, diese soll die TF-IDF-Matrix für alle Artikel und die Anzahl der Topics übergeben bekommen und die Matrizen $U$ und $V^\\top$ berechnen und zurückgeben.\n",
    "\n",
    "Diese Funktion wird auf Moodle abgefragt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from numpy.typing import ArrayLike\n",
    "import numpy as np\n",
    "\n",
    "def compute_matrix_factorization(tfidf: ArrayLike, n_topics: int = 2) -> tuple[ArrayLike, ArrayLike]:\n",
    "    \"\"\"Computes the matrix factorization (U @ V.T) of the TF-IDF matrix using NMF.\n",
    "\n",
    "    Args:\n",
    "        tfidf (ArrayLike): The TF-IDF matrix.\n",
    "        n_topics (int, optional): The number of topics to consider during matrix factorization. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        tuple[ArrayLike, ArrayLike]: The U and V.T matrices of the factorization.\n",
    "    \"\"\"\n",
    "    # Ensure the TF-IDF matrix is non-negative and in dense format\n",
    "    if isinstance(tfidf, np.ndarray):\n",
    "        dense_tfidf = tfidf\n",
    "    else:\n",
    "        dense_tfidf = tfidf.toarray()  # Convert sparse matrix to dense format\n",
    "\n",
    "    if np.any(dense_tfidf < 0):\n",
    "        raise ValueError(\"TF-IDF matrix contains negative values. Ensure the input is non-negative.\")\n",
    "\n",
    "    # Initialize and perform Non-Negative Matrix Factorization (NMF)\n",
    "    nmf = NMF(\n",
    "        n_components=n_topics,\n",
    "        init='nndsvda',  # Suitable initialization for sparse/non-negative matrices\n",
    "        random_state=42,\n",
    "        max_iter=300  # Default, but explicitly stated for clarity\n",
    "    )\n",
    "    U = nmf.fit_transform(dense_tfidf)  # U matrix (document-topic representation)\n",
    "    V_T = nmf.components_  # V^T matrix (topic-term representation)\n",
    "\n",
    "    return U, V_T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 3\n",
    "U, V_T = compute_matrix_factorization(tfidf, n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3\n",
    "\n",
    "Nachdem die Matrixfaktorisierung nun erfolgreich berechnet wurde, kann das Ergebnis dieser Berechnungen genutzt werden, um die Topics der Nachrichten zu bestimmen.\n",
    "\n",
    "Implementieren Sie hierzu die Funktion `get_sentence_categories`, die die Matrizen $U$ und $V^\\top$ übergeben bekommt und für jeden Artikel das zugehörige Topic bestimmt.\n",
    "\n",
    "**Hinweis**: Die Funktion benötigt nur eine der beiden Matrizen, um die gesuchte Information zu bestimmen. Denken Sie selber darüber nach, wo diese Information zu finden ist.\n",
    "\n",
    "Diese Funktion wird auf Moodle abgefragt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "def get_sentence_categories(U: ArrayLike, V_T: ArrayLike) -> ArrayLike:\n",
    "    \"\"\"Finds the category of each sentence based on the U matrix.\n",
    "\n",
    "    Args:\n",
    "        U (ArrayLike): The U matrix of the matrix factorization.\n",
    "        V_T (ArrayLike): The V_T matrix of the matrix factorization.\n",
    "\n",
    "    Returns:\n",
    "        ArrayLike: The index of the topic each sentence belongs to.\n",
    "    \"\"\"\n",
    "    # Determine the topic with the highest weight for each article (row in U)\n",
    "    sentence_categories = np.argmax(U, axis=1)\n",
    "    return sentence_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_categories = get_sentence_categories(U, V_T)\n",
    "print(\"Categories assigned to some sentences:\")\n",
    "for i, sentence in enumerate(data[:10]):\n",
    "    print(f\"Sentence: '{sentence}' - Category: {sentence_categories[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4\n",
    "\n",
    "Zusätzlich können wir die Topics anhand der Wörter charakterisieren, die mit einem Topic stark assoziiert sind.\n",
    "\n",
    "Implementieren Sie hierzu die Funktion `get_most_important_topic_words`, die die Matrizen $U$ und $V^\\top$, die Namen der Features `words` und die Anzahl der Wörter pro Topic `n_words` übergeben bekommt und für jedes Topic die `n_words` wichtigsten Wörter bestimmt.\n",
    "\n",
    "**Hinweis**: Die Funktion benötigt nur eine der beiden Matrizen, um die gesuchte Information zu bestimmen. Denken Sie selber darüber nach, wo diese Information zu finden ist.\n",
    "\n",
    "Diese Funktion wird auf Moodle abgefragt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "def get_most_important_topic_words(U: ArrayLike, V_T: ArrayLike, words: ArrayLike, n_words: int) -> list[list[str]]:\n",
    "    \"\"\"Return the most important words for each topic.\n",
    "\n",
    "    Args:\n",
    "        U (ArrayLike): The U matrix (m x k) of the non-negative matrix factorization.\n",
    "        V_T (ArrayLike): The V.T matrix (k x n) of the non-negative matrix factorization.\n",
    "        words (ArrayLike): The feature names (n x 1) associated with the TF-IDF matrix.\n",
    "        n_words (int): The number of words to return for each topic.\n",
    "\n",
    "    Returns:\n",
    "        list[list[str]]: A list of lists of strings where each inner list contains\n",
    "        the most important words for a topic sorted from most important to least important.\n",
    "    \"\"\"\n",
    "    topic_words = []\n",
    "\n",
    "    # Für jedes Topic\n",
    "    for i in range(V_T.shape[0]):\n",
    "        # Extrahiere die Gewichtungen für das aktuelle Topic\n",
    "        topic_weights = V_T[i]\n",
    "        \n",
    "        # Holen Sie sich die Indizes der n wichtigsten Wörter\n",
    "        top_indices = np.argsort(topic_weights)[::-1][:n_words]\n",
    "        \n",
    "        # Ordne die Wörter den Indizes zu und füge sie zur Liste hinzu\n",
    "        topic_words.append([words[idx] for idx in top_indices])\n",
    "    \n",
    "    return topic_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most important words for each topic:\")\n",
    "for topic_idx, topic in enumerate(get_most_important_topic_words(U, V_T, words, 20)):\n",
    "    print(f\"Topic {topic_idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud (keine Aufgabe)\n",
    "\n",
    "Alternativ können Sie die wichtigsten Wörter pro Topic auch als *word cloud* darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(U: ArrayLike, V_T: ArrayLike, topic_idx: int, words: ArrayLike):\n",
    "    \"\"\"Plots a word cloud of the most important words for a given topic.\n",
    "\n",
    "    Args:\n",
    "        U (ArrayLike): The U matrix (m x k) of the non-negative matrix factorization.\n",
    "        V_T (ArrayLike): The V.T matrix (k x n) of the non-negative matrix factorization.\n",
    "        topic_idx (int): The index of the topic to plot the word cloud for.\n",
    "        words (ArrayLike): The feature names (n x 1) associated with the TF-IDF matrix.\n",
    "    \"\"\"\n",
    "    topic = V_T[topic_idx]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(zip(words, topic)))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Topic {topic_idx}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx in range(min(n_topics, 10)):\n",
    "    plot_word_cloud(U, V_T, topic_idx, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 5\n",
    "\n",
    "Zuletzt wollen wir betrachten, welche Wörter häufig in Artikeln eines bestimmten Topics gefunden werden.\n",
    "\n",
    "Implementieren Sie hierzu die Funktion `visualize_words_by_topic`, die die Matrizen $U$ und $V^\\top$, die Namen der Features `words`, die Indizes des latenten Raums, die auf der $x$ und $y$ Achse visualisiert werden sollen `topic_x`, `topic_y` und eine Anzahl an Wörtern `n_words` übergeben bekommt und die Wörter mit dem größten Einfluss auf die Topics visualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_words_by_topic(U: ArrayLike, V_T: ArrayLike, words: ArrayLike, topic_x:int=0, topic_y:int=1, n_words:int=50):\n",
    "    \"\"\"Visualizes the contributions of words to two topics in a scatter plot.\n",
    "\n",
    "    Args:\n",
    "        U (ArrayLike): The U matrix (m x k) of the non-negative matrix factorization\n",
    "        V_T (ArrayLike): The V.T matrix (k x n) of the non-negative matrix factorization.\n",
    "        words (ArrayLike): The feature names (n x 1) associated with the TF-IDF matrix.\n",
    "        topic_x (int, optional): The latent space index visualized on the x-axis. Defaults to 0.\n",
    "        topic_y (int, optional): The latent space index visualized on the y-axis. Defaults to 1.\n",
    "        n_words (int, optional): The number of words that should be visualized. Defaults to 50.\n",
    "    \"\"\"\n",
    "    # Get contributions for the chosen topics only\n",
    "    # Combine contributions from both topics into a tuple (x, y) for sorting\n",
    "    contributions = np.array([(V_T[topic_x, i], V_T[topic_y, i]) for i in range(V_T.shape[1])])\n",
    "    \n",
    "    # Get the top N indices based on the contribution of the specified topics\n",
    "    top_indices = np.argsort(np.linalg.norm(contributions, axis=1))[-n_words:]\n",
    "    \n",
    "    # Get the corresponding words and their contributions for the selected topics\n",
    "    top_words = words[top_indices]\n",
    "    \n",
    "    # Get the contributions for the chosen topics\n",
    "    x_values = V_T[topic_x, top_indices]  # Contributions to topic X\n",
    "    y_values = V_T[topic_y, top_indices]  # Contributions to topic Y\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_values, y_values, alpha=0.7)\n",
    "\n",
    "    # Annotate each point with the corresponding word\n",
    "    for i, word in enumerate(top_words):\n",
    "        plt.annotate(word, (x_values[i], y_values[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n",
    "\n",
    "    plt.title(f\"Word Contributions to Topic {topic_x} vs Topic {topic_y}\")\n",
    "    plt.xlabel(f\"Contribution to Topic {topic_x}\")\n",
    "    plt.ylabel(f\"Contribution to Topic {topic_y}\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_words_by_topic(U, V_T, words, topic_x=0, topic_y=1, n_words=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
