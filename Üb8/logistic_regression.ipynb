{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# TODO:\n",
    "# What do you need from sklearn? All of it is there somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_withfeatures_pandas(dataset_path):\n",
    "    \"\"\"This function loads CSV datasets using the read_csv method of the pandas library.\n",
    "    The CSV is expected to be comma-separated, while individual examples are separated by new line.\n",
    "    All but the last column are expected to be features, the last column is parsed as output variable.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: The path to the dataset\n",
    "\n",
    "    Returns:\n",
    "        X: The data points as data matrix with shape (n_samples, n_features)\n",
    "        y: The labels of the data points\n",
    "        feature_names: The names of the features as list of strings\n",
    "    \"\"\"\n",
    "   \n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    X = df.iloc[:, :-1].to_numpy()\n",
    "    y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "    feature_names = df.iloc[:, :-1].columns.tolist()\n",
    "\n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale_data(X_train, X_test):\n",
    "    \"\"\"Standard scales data by subtracting the mean and scaling to unit variance.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training data points as matrix (n_samples, n_features)\n",
    "        X_test: Test data points as matrix (n_samples, n_features)\n",
    "\n",
    "    Returns:\n",
    "        X_train: Scaled training data points as matrix (n_samples, n_features)\n",
    "        X_test: Scaled testing data points as matrix (n_samples, n_features)\n",
    "        std_scaler: The Scaler used to scale X_train and X_test\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: It won't hurt to scale your data in most cases\n",
    "    # How do you do that? Choose from the methods you learned\n",
    "    # Hint: The data was not artificial changed to create certain\n",
    "    # advantages of a scaler, they are taken from source unprocessed.\n",
    "\n",
    "    return X_train, X_test, std_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_boundary(classifier, X_test, y_test, feature_names=None, class_names=None, fig_title=None):\n",
    "    \"\"\"Visualizes the decision boundaries determined by the classifier and the test data points.\n",
    "\n",
    "    Args:\n",
    "        classifier: The classifier whose decision boundaries should be visualized\n",
    "        X_test: The test data points\n",
    "        y_test: The corresponding test data labels\n",
    "        feature_names: The names of the features\n",
    "        class_names: The names of the classes\n",
    "        fig_title: The title of the figure\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D moon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# load data\n",
    "# build test set\n",
    "# train classifiers\n",
    "# visualize decision boundaries\n",
    "# look at metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional if you want to try LinearSVC: default value for class is dual=True, but this is not suitable for our examples\n",
    "classifier = LinearSVC(dual=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# load data\n",
    "# build test set\n",
    "# train classifiers -> this is a multi class problem!\n",
    "# visualize decision boundaries\n",
    "# look at metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit card fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# load data\n",
    "# build test set\n",
    "# train classifiers\n",
    "# look at metrics\n",
    "# Precision-Recall-Curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fc36ab0d55a6a226ae882655424b723ba299a7c7e2b24a1d4fe088de8ed7471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
